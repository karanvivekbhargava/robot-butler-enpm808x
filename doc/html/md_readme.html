<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>My Project: readme</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">My Project
   &#160;<span id="projectnumber">Midsemester project of Karan Vivek Bhargava</span>
   </div>
   <div id="projectbrief">ENPM808X Robot Butler</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('md_readme.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">readme </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1 align="center">ENPM808X - <a class="el" href="class_robot.html" title="Class for robot. ">Robot</a> Butler </h1>
<p><a href="https://travis-ci.org/karanvivekbhargava/robot-butler-enpm808x"></a> <a href="https://coveralls.io/github/karanvivekbhargava/robot-butler-enpm808x?branch=master"></a> <a href="https://opensource.org/licenses/MIT"></a> </p>
<div class="image">
<img src="https://cdn.andnowuknow.com/mainStoryImage/robot_butler_aug_2014_banner.jpg" />
</div>
<p> Reference for image: <a href="http://www.savioke.com/">link</a> </p>
<h2>Project Overview</h2>
<p>The Butler product by Acme Robotics is one of its flagship products. It performs best for an environment where things are to be transported to and fro from one area to another. Equipped with a 16MP camera and the best of class custom lidar sensor, its the best offering one can hope for. The butler has intelligent algorithms running under its hood which allow it to percieve its environment by using these sensors. This allows the butler to avoid hitting obstacles and helps it serve you better.</p>
<h2>New Feature List</h2>
<p>The new offerings in software are included below.</p><ul>
<li>Estimation of object distances using camera data: While other companies are defining state of the art algorithms on the road, the butler does it indoors. Making the robots less likely to crash into objects than the competitors.</li>
<li>Using lidar to map your environment: The butler records its surroundings in 3D so that it can see obstacles before they hit it.</li>
<li>Advanced data fusion algorithms: This robot is cool but don't be fooled by its innocent appearance, it works super hard on the inside to crunch numbers faster than ever. Combining the data from both the sensors, it gets a better estimate on how it should move.</li>
<li>Path Planning: Using our custom sensors and the fusion technique, the butler can plan the path better to avoid obstacles <hr/>
 <h2>The Camera</h2>
</li>
</ul>
<p>The butler has a 16MP front facing camera. Its camera module consists of an FPGA which can perform custom algorithms at a mind boggling pace. Once the input image arrives, the module does a perspective transform on it. This gives us a birds eye view which is then passed to a thresholder. The binarized image from the thresholder is then used to calculate the probabilities of hitting the nearby obstacles. We use a gaussian probability distribution to compute the same.</p>
<p>The images below show how the camera module is manupulating the data to translate it into a probability. The left image is the input, the center image is the perspective warped image and the right image is the thresholded image after warping. After this I'm checking the distances to obstacles with different headding directions. I can use these distances to obtain probabilties using a gaussian distribution. </p>
<div class="image">
<img src="data/output/1_original_resized.png"  width="270" height="200"/>
</div>
 <div class="image">
<img src="data/output/3_output_warped.png"  width="270" height="200"/>
</div>
 <div class="image">
<img src="data/output/4_output_warped_thresholded.png"  width="270" height="200"/>
</div>
 <h2>The Lidar</h2>
<p>The lidar gives a three dimensional point cloud representation of its surroundings. It uses this information and 'flattens' it out. This results in all the points being in some eucledean plane and the robot being the origin. It computes the distances from the obstacles and returns gaussian probabilitites to all the possible heading directions of the robot.</p>
<p>Example: Consider that we have a point from the point cloud as below. PS: The points are preprocessed to be in the heading directions that we are going to consider. </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;p = (1, 2, 3)</div></div><!-- fragment --><p> The points will be flattened at first, this results in the representation of the points on the ground (euclidean plane). </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;p_flattened = (1, 2)</div></div><!-- fragment --><p> We will then compute the distances of the points from the origin (this is where the robot will be at all times) </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;d = sqrt(1^2 + 2^2)</div></div><!-- fragment --><p> We then turn these distances into the probabilities of hitting an obstacle by </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;Probability = C*e^((d-mean)/(2*variance))</div></div><!-- fragment --><p> Where C is a normalization constant, mean is 0 and variance is tuned according to the data. The lidar outputs these probabilites.</p>
<h2>Sensor Fusion</h2>
<p>After the camera and lidar do the hard work of putting the information in a sensible format, the sensor fusion module takes the two readings and selects the higher probability of the two, for each heading direction. Although this might result in some noisy outputs, it avoids obstacles effectively.</p>
<p>Example: If say we have the incoming probabilities given below where the probabilities correspond to heading directions -50, -30, -10, 10, 30, 50 degrees from current heading direction. </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;P_image = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6];</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;P_lidar = [0.6, 0.5, 0.4, 0.3, 0.2, 0,1];</div></div><!-- fragment --><p> Fused probability is given by </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;P_fusion = max(P_image, P_lidar)</div></div><!-- fragment --><p> Which gives us, </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;P_fusion = [0.6, 0.5, 0.4, 0.4, 0.5, 0.6];</div></div><!-- fragment --><h2>Path Planning</h2>
<p>The path planner uses the fused sensor output to determine what should be the next heading direction. This is done by selecting the heading direction which results in the least probability of hitting any obstacles.</p>
<p>Example: If the incoming fused probabilities are as given below </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;P_fused = [0.1, 0.2, 0.0, 0.01, 0.1, 0,3];</div></div><!-- fragment --><p> Then the path planner can be written mathematically as </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;direction = argmin_x( P_fused[x] )</div></div><!-- fragment --><p> Then the output of the path planner would be </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;direction = 2 (index of 0.0)</div></div><!-- fragment --><p>The robot later converts this direction into an angle by using</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;angle = (direction - 2.5)*20.0;</div></div><!-- fragment --><h2>The <a class="el" href="class_robot.html" title="Class for robot. ">Robot</a></h2>
<p>The robot is the class which uses all the other modules and creates instances of a camera, lidar, sensor fusion module and path planning module. It then uses the modules to run. </p>
<div class="image">
<img src="UML/Activity_Diagram_v2.jpg" />
</div>
 <hr/>
 <h2>Results</h2>
<p>The software takes the camera image as well as the processed point cloud. I've configured the robot for three pictures by generating the respective lidar inputs.</p>
<p>Following are the results</p><ol type="1">
<li>Obstacle on the left </li>
</ol>
<div class="image">
<img src="data/image_left.jpg"  width="270" height="200"/>
</div>
 <div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;Output:</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;[OK] Diagnostics are fine.</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;Rotate robot counter clockwise by -10</div></div><!-- fragment --><ol type="1">
<li>Obstacle in the center </li>
</ol>
<div class="image">
<img src="data/image_center.jpg"  width="270" height="200"/>
</div>
 <div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;Output:</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;[OK] Diagnostics are fine.</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;Rotate robot counter clockwise by 50</div></div><!-- fragment --><ol type="1">
<li>Obstacle on the right </li>
</ol>
<div class="image">
<img src="data/image_right.jpg"  width="270" height="200"/>
</div>
 <div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;Output:</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;[OK] Diagnostics are fine.</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;Rotate robot counter clockwise by -50</div></div><!-- fragment --> <hr/>
 <h2>Solo Iterative Process Overview</h2>
<p>Click this link to view the product backlog, time sheets, defect logs and release backlog - <a href="https://docs.google.com/spreadsheets/d/1WOvV6iL4gGOF8Qacwj2R3Lom71wziKXEf_UEhdGfOuY/edit?usp=sharing">link</a></p>
<p>Care has been taken to design the SIP tasks such that they have a direct relation to the previous tasks. This helps in better time estimation. For instance, the change in time taken for stub implementation is proportional to the change in time taken to implement the methods. This gave me a good idea to rethink about the allotment of time for future tasks. </p><hr/>
 <h2>Dependencies</h2>
<p>The butler software stack has the following dependencies:</p><ul>
<li>cmake</li>
<li>googletest</li>
<li>opencv</li>
</ul>
<p>To install opencv, follow the instructions on <a href="https://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html">link</a></p>
<p>## How to build - standard install via command-line </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;git clone --recursive https://github.com/karanvivekbhargava/robot-butler-enpm808x</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;cd &lt;path to repository&gt;</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;mkdir build</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;cd build</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;cmake ..</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;make</div></div><!-- fragment --><h2>How to run demo</h2>
<p>After following the installation instructions above, you can try three different images from the data folder by specifying the argument with the program</p>
<p>To run for image_left.jpg, kindly enter the function below </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;./app/shell-app left</div></div><!-- fragment --><p> To run for image_center.jpg, kindly enter the function below </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;./app/shell-app center</div></div><!-- fragment --><p> To run for image_right.jpg, kindly enter the function below </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;./app/shell-app right</div></div><!-- fragment --><h2>How to run tests</h2>
<p>After following the building instructions, run the command below </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;./test/cpp-test</div></div><!-- fragment --><h2>How to generate documentation</h2>
<p>Although the repository contains the documentation, if you'd still like to generate it then follow the instructions below.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;sudo apt-get install doxygen</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;sudo apt-get install doxywizard</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;doxywizard</div></div><!-- fragment --><p>Once doxywizard is open, select the workspace as the repository. Fill in the details as required and set the source code folder to the repository as well. Create a new folder in the repository and select that as the destination directory. Proceed with the default settings and generate the documentation.</p>
<p>Alternatively, you can run</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;doxygen -g doxygenconfig</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;doxygen doxygenconfig</div></div><!-- fragment --> </div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.11 </li>
  </ul>
</div>
</body>
</html>
